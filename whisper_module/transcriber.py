from tkinter import messagebox
from exceptiongroup import catch
from resemblyzer import VoiceEncoder
from sklearn.cluster import KMeans
from whisper_module.speaker_identifier import get_speaker_db, identify_by_embedding
from typing import List, Dict, Any
import os
import math
import torch
import torchaudio
from pydub import AudioSegment
from faster_whisper import WhisperModel

SPEAKER_DB = "speaker_db"

class Transcribe:
    def __init__(self):
        self.model_name = "base"
        self.speaker_count = 2
        self.model = None
        pass

    def execute(self,
               audio_path: str,
               chunk_length_s: int = 300
               ) -> List[Dict[str, Any]]:
        """
           ê¸´ ì˜¤ë””ì˜¤(audio_path)ë¥¼ chunk_length_s(ì´ˆ) ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ë’¤,
           ê° chunkë§ˆë‹¤ ì „ì‚¬â†’ì„ë² ë”©â†’KMeansâ†’í™”ì ì‹ë³„ì„ ìˆ˜í–‰í•˜ê³ ,
           ì „ì²´ ê²°ê³¼ë¥¼ í•©ì³ ë¦¬í„´í•©ë‹ˆë‹¤.
           """

        # 1) GPU ë©”ëª¨ë¦¬ ìºì‹œ ë¹„ìš°ê¸°
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # 2) WhisperModel ë¡œë“œ (GPU â†’ CPU ìë™ fallback)
        self.model, device = load_whisper_model(self.model_name)
        model = self.model

        # 3) ì˜¤ë””ì˜¤ë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì–»ê¸°
        chunk_paths = split_audio_to_chunks(audio_path, chunk_length_s)
        if not chunk_paths:
            raise ValueError("Audio splitting resulted in no chunks.")

        print(f"ğŸ”€ ì´ {len(chunk_paths)}ê°œ ì¡°ê°ìœ¼ë¡œ ë¶„í• : {chunk_paths}")

        # 4) Resemblyzerìš© ìŒì„± DBì™€ encoder ì¤€ë¹„
        encoder = VoiceEncoder(device="cpu")
        speaker_db = get_speaker_db(encoder)

        # 5) ëª¨ë“  chunkë¥¼ ì²˜ë¦¬í•˜ë©° ìµœì¢… ê²°ê³¼ ëˆ„ì 
        final_results: List[Dict[str, Any]] = []
        offset_s = 0.0  # ì´ì „ chunkê¹Œì§€ ëˆ„ì ëœ ì‹œê°„(ì´ˆ)

        for idx, chunk_path in enumerate(chunk_paths, start=1):
            print(f"--- Chunk {idx}/{len(chunk_paths)} ì „ì‚¬ ì‹œì‘: {chunk_path} (ì˜¤í”„ì…‹ {offset_s}s)")

            # ì „ì‚¬ ì „ GPU ìºì‹œ ë¹„ìš°ê¸°
            import gc
            gc.collect()
            if device == "cuda":
                torch.cuda.empty_cache()

            # 5.1) chunk ì „ì‚¬ (ë¡œì»¬ ì‹œê°„)
            segments, _ = model.transcribe(chunk_path)
            segments = list(segments)
            if not segments:
                print(f"--- Chunk {idx}: ì „ì‚¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ, offsetë§Œ ì¦ê°€")
                chunk_audio = AudioSegment.from_file(chunk_path)
                offset_s += len(chunk_audio) / 1000.0
                continue

            # 5.2) chunk íŒŒì¼ ë¡œë“œí•˜ì—¬ ì„ë² ë”© ì¶”ì¶œ ì¤€ë¹„
            waveform, sr = torchaudio.load(chunk_path)
            waveform = waveform.mean(dim=0).numpy()

            chunk_embeddings = []
            chunk_segdata = []

            for seg in segments:
                st, ed, txt = seg.start, seg.end, seg.text
                start_frame = int(st * sr)
                end_frame = int(ed * sr)
                segment_wav = waveform[start_frame:end_frame]
                if len(segment_wav) < sr * 0.3:
                    continue
                emb = encoder.embed_utterance(segment_wav)
                chunk_embeddings.append(emb)
                # offsetì„ ë”í•´ ì „ì²´ ê¸°ì¤€ ì‹œê°„ìœ¼ë¡œ ë³€í™˜
                chunk_segdata.append({
                    "start": st + offset_s,
                    "end": ed + offset_s,
                    "text": txt
                })

            if not chunk_embeddings:
                print(f"--- Chunk {idx}: ì„ë² ë”© ì¶”ì¶œëœ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ, offsetë§Œ ì¦ê°€")
                chunk_audio = AudioSegment.from_file(chunk_path)
                offset_s += len(chunk_audio) / 1000.0
                continue

            # 5.3) ì´ chunk ë‚´ì—ì„œë§Œ KMeans í´ëŸ¬ìŠ¤í„°ë§
            print(f"--- Chunk {idx}: ğŸ”€ {len(chunk_embeddings)}ê°œ ì„ë² ë”©ìœ¼ë¡œ KMeans({self.speaker_count}) ì‹¤í–‰")
            kmeans = KMeans(n_clusters=self.speaker_count, random_state=0).fit(chunk_embeddings)
            labels = kmeans.labels_

            # 5.4) í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ë³„ í™”ì ì´ë¦„ ë§¤í•‘
            cluster_to_name: Dict[int, str] = {}
            for cidx, center in enumerate(kmeans.cluster_centers_):
                name = identify_by_embedding(center, speaker_db, threshold=0.5)
                cluster_to_name[cidx] = name

            # 5.5) chunk_segdataì™€ labelsë¥¼ ê²°í•©í•´ ìµœì¢… ê²°ê³¼ì— ì¶”ê°€
            for seg_info, cidx in zip(chunk_segdata, labels):
                final_results.append({
                    "speaker": cluster_to_name.get(cidx, "Unknown"),
                    "start": seg_info["start"],
                    "end": seg_info["end"],
                    "text": seg_info["text"]
                })

            # 5.6) ì´ chunkì˜ ê¸¸ì´ë§Œí¼ offset ì¦ê°€
            chunk_audio = AudioSegment.from_file(chunk_path)
            offset_s += len(chunk_audio) / 1000.0

            print(f"--- Chunk {idx} ì™„ë£Œ, ëˆ„ì  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(final_results)}, offset â†’ {offset_s}s")
        print(f"--- Done Transcriber")

        # 6) ì„ì‹œ chunk íŒŒì¼ ì‚­ì œ
        print("â–¶ Cleanup ì‹œì‘: tmp_chunks ì‚­ì œ ì „")  # (A)
        for p in chunk_paths:
            try:
                os.remove(p)
            except Exception as e:
                print(e)
                pass
        try:
            os.rmdir("tmp_chunks")
        except Exception as e:
            print(e)
            pass
        print("â–¶ Cleanup: tmp_chunks ì‚­ì œ ì™„ë£Œ")  # (B)

        # 7) GPU ë©”ëª¨ë¦¬ ì¡ê³  ìˆëŠ” ê°ì²´ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ë¹„ìš°ê¸°
        print("â–¶ Cleanup: GPU ìºì‹œ ë¹„ìš°ê¸° ì „")  # (C)
        try:
            import gc
            gc.collect()
            print("GC Clean")
            torch.cuda.empty_cache()
            print("â–¶ Cleanup: torch.cuda.empty_cache() í˜¸ì¶œ ì™„ë£Œ")  # (D)
        except Exception as e:
            print(f"Cleanup ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            pass

        print(f"--- GPU Clear")
        print(f"â–¶ ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜: {len(final_results)}")
        return final_results

def split_audio_to_chunks(input_path: str, chunk_length_s: int = 300) -> list[str]:
    """
    input_path(mp3/wav)ì„ chunk_length_s(ì´ˆ) ë‹¨ìœ„ë¡œ ìë¥´ê³ ,
    ì„ì‹œ WAV íŒŒì¼ë¡œ ì €ì¥í•œ ë’¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.
    """
    chunk_paths = []
    try:
        audio = AudioSegment.from_file(input_path)
        total_ms = len(audio)
        chunk_ms = chunk_length_s * 1000

        os.makedirs("tmp_chunks", exist_ok=True)
        for start_ms in range(0, total_ms, chunk_ms):
            end_ms = min(start_ms + chunk_ms, total_ms)
            chunk = audio[start_ms:end_ms]
            if len(chunk) <= 0:
                continue
            start_s = start_ms // 1000
            end_s = math.ceil(end_ms / 1000)
            chunk_name = f"tmp_chunks/chunk_{start_s}_{end_s}.wav"
            chunk.export(chunk_name, format="wav")
            chunk_paths.append(chunk_name)
    except Exception as e:
        print("split_audio_to_chunks ì˜¤ë¥˜:", e)

    # ë§Œì•½ í•œ ê°œë„ chunkê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì›ë³¸ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ chunkë¡œ ê°„ì£¼
    if not chunk_paths:
        try:
            os.makedirs("tmp_chunks", exist_ok=True)
            fallback_name = "tmp_chunks/chunk_0_0.wav"
            audio.export(fallback_name, format="wav")
            chunk_paths.append(fallback_name)
        except Exception as ex:
            print("fallback chunk ìƒì„± ì‹¤íŒ¨:", ex)

    return chunk_paths


def load_whisper_model(model_size: str):
    """
    GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ WhisperModelì„ ì‹œë„í•´ì„œ ë¡œë“œí•˜ê³ ,
    ì‹¤íŒ¨í•˜ê±°ë‚˜ GPUê°€ ì—†ìœ¼ë©´ CPUë¡œ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    use_cuda = torch.cuda.is_available()
    if use_cuda:
        for compute in ("float16", "int8"):
            try:
                model = WhisperModel(model_size, device="cuda", compute_type=compute)
                print(f"â–¶ WhisperModel('{model_size}') loaded on GPU (compute_type={compute})")
                return model, "cuda"
            except Exception as e:
                print(f"âš  GPU ë¡œë“œ ì‹¤íŒ¨ (compute_type={compute}): {e}")
        print("â–¶ GPUë¡œ ëª¨ë‘ ì‹¤íŒ¨, CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

    model = WhisperModel(model_size, device="cpu", compute_type="int8")
    print(f"â–¶ WhisperModel('{model_size}') loaded on CPU")
    return model, "cpu"


if __name__ == "__main__":
    path = "meeting.wav"
    results = transcribe_and_identify(path, model_size="base", num_speakers=2, chunk_length_s=300)
    with open("transcript.txt", "w", encoding="utf-8") as f:
        for seg in results:
            f.write(f"[{seg['speaker']}] {seg['text']}\n")
    print("âœ… Written to transcript.txt")
